{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ba0fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Let's do some excercise today.\",\n",
    "\"Are you interested in running, I love it!\",\n",
    "\"what about swimming, Yeaah woh fun!\"\n",
    "\"These are basic text to learn how cleaning works.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6db0a",
   "metadata": {},
   "source": [
    "## Tokenizing text into bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a524a1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Let', \"'s\", 'do', 'some', 'excercise', 'today', '.'], ['Are', 'you', 'interested', 'in', 'running', ',', 'I', 'love', 'it', '!'], ['what', 'about', 'swimming', ',', 'Yeaah', 'woh', 'fun', '!', 'These', 'are', 'basic', 'text', 'to', 'learn', 'how', 'cleaning', 'works', '.']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfb455",
   "metadata": {},
   "source": [
    "## Removing punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fadc791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Let', 's', 'do', 'some', 'excercise', 'today'], ['Are', 'you', 'interested', 'in', 'running', 'I', 'love', 'it'], ['what', 'about', 'swimming', 'Yeaah', 'woh', 'fun', 'These', 'are', 'basic', 'text', 'to', 'learn', 'how', 'cleaning', 'works']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd572b",
   "metadata": {},
   "source": [
    "## Cleaning text of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f369ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Let', 'excercise', 'today'], ['Are', 'interested', 'running', 'I', 'love'], ['swimming', 'Yeaah', 'woh', 'fun', 'These', 'basic', 'text', 'learn', 'cleaning', 'works']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89918448",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43e41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['let', 'let', 'Let', 'excercis', 'excercis', 'excercise', 'today', 'today', 'today'], ['are', 'are', 'Are', 'interest', 'interest', 'interested', 'run', 'run', 'running', 'i', 'i', 'I', 'love', 'love', 'love'], ['swim', 'swim', 'swimming', 'yeaah', 'yeaah', 'Yeaah', 'woh', 'woh', 'woh', 'fun', 'fun', 'fun', 'these', 'these', 'These', 'basic', 'basic', 'basic', 'text', 'text', 'text', 'learn', 'learn', 'learn', 'clean', 'clean', 'cleaning', 'work', 'work', 'work']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/mubinaarastu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "        final_doc.append(snowball.stem(word))\n",
    "        final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
